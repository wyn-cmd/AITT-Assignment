{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef30401f-a091-4187-b170-8840edb9b7c4",
   "metadata": {},
   "source": [
    "## 1. Importing the relavent functions to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c35fe8-fb95-4068-bb70-b0b7839c88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c83de-4f18-436e-92a8-4f68a329b21b",
   "metadata": {},
   "source": [
    "### 2. Reading the logs as well as the event IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05151274-6eff-4561-b832-e424a6742e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'HDFS.log'\n",
    "eid_df = pd.read_csv('HDFS.log_templates.csv')\n",
    "\n",
    "with open(path_to_file) as input_file:\n",
    "    # hdfs_head = [next(input_file) for _ in range(300000)]\n",
    "    hdfs_head = input_file.readlines()\n",
    "    # hdfs = input_file.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce4d27-03c2-4490-ad76-22bb86e188f0",
   "metadata": {},
   "source": [
    "### 3. Formatting the event IDs for regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4496f738-a4f7-456d-aa53-c634e51207c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>regex_pattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E1</td>\n",
       "      <td>[*]Adding an already existing block[*]</td>\n",
       "      <td>(.*?)Adding an already existing block(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E2</td>\n",
       "      <td>[*]Verification succeeded for[*]</td>\n",
       "      <td>(.*?)Verification succeeded for(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E3</td>\n",
       "      <td>[*]Served block[*]to[*]</td>\n",
       "      <td>(.*?)Served block(.*?)to(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E4</td>\n",
       "      <td>[*]Got exception while serving[*]to[*]</td>\n",
       "      <td>(.*?)Got exception while serving(.*?)to(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E5</td>\n",
       "      <td>[*]Receiving block[*]src:[*]dest:[*]</td>\n",
       "      <td>(.*?)Receiving block(.*?)src:(.*?)dest:(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E6</td>\n",
       "      <td>[*]Received block[*]src:[*]dest:[*]of size[*]</td>\n",
       "      <td>(.*?)Received block(.*?)src:(.*?)dest:(.*?)of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E7</td>\n",
       "      <td>[*]writeBlock[*]received exception[*]</td>\n",
       "      <td>(.*?)writeBlock(.*?)received exception(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E8</td>\n",
       "      <td>[*]PacketResponder[*]for block[*]Interrupted[*]</td>\n",
       "      <td>(.*?)PacketResponder(.*?)for block(.*?)Interru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E9</td>\n",
       "      <td>[*]Received block[*]of size[*]from[*]</td>\n",
       "      <td>(.*?)Received block(.*?)of size(.*?)from(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E10</td>\n",
       "      <td>[*]PacketResponder[*]Exception[*]</td>\n",
       "      <td>(.*?)PacketResponder(.*?)Exception(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>E11</td>\n",
       "      <td>[*]PacketResponder[*]for block[*]terminating[*]</td>\n",
       "      <td>(.*?)PacketResponder(.*?)for block(.*?)termina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>E12</td>\n",
       "      <td>[*]:Exception writing block[*]to mirror[*]</td>\n",
       "      <td>(.*?):Exception writing block(.*?)to mirror(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E13</td>\n",
       "      <td>[*]Receiving empty packet for block[*]</td>\n",
       "      <td>(.*?)Receiving empty packet for block(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E14</td>\n",
       "      <td>[*]Exception in receiveBlock for block[*]</td>\n",
       "      <td>(.*?)Exception in receiveBlock for block(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E15</td>\n",
       "      <td>[*]Changing block file offset of block[*]from[...</td>\n",
       "      <td>(.*?)Changing block file offset of block(.*?)f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>E16</td>\n",
       "      <td>[*]:Transmitted block[*]to[*]</td>\n",
       "      <td>(.*?):Transmitted block(.*?)to(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>E17</td>\n",
       "      <td>[*]:Failed to transfer[*]to[*]got[*]</td>\n",
       "      <td>(.*?):Failed to transfer(.*?)to(.*?)got(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>E18</td>\n",
       "      <td>[*]Starting thread to transfer block[*]to[*]</td>\n",
       "      <td>(.*?)Starting thread to transfer block(.*?)to(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>E19</td>\n",
       "      <td>[*]Reopen Block[*]</td>\n",
       "      <td>(.*?)Reopen Block(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>E20</td>\n",
       "      <td>[*]Unexpected error trying to delete block[*]B...</td>\n",
       "      <td>(.*?)Unexpected error trying to delete block(....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>E21</td>\n",
       "      <td>[*]Deleting block[*]file[*]</td>\n",
       "      <td>(.*?)Deleting block(.*?)file(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>E22</td>\n",
       "      <td>[*]BLOCK* NameSystem[*]allocateBlock:[*]</td>\n",
       "      <td>(.*?)BLOCK(.*?) NameSystem(.*?)allocateBlock:(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>E23</td>\n",
       "      <td>[*]BLOCK* NameSystem[*]delete:[*]is added to i...</td>\n",
       "      <td>(.*?)BLOCK(.*?) NameSystem(.*?)delete:(.*?)is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>E24</td>\n",
       "      <td>[*]BLOCK* Removing block[*]from neededReplicat...</td>\n",
       "      <td>(.*?)BLOCK(.*?) Removing block(.*?)from needed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>E25</td>\n",
       "      <td>[*]BLOCK* ask[*]to replicate[*]to[*]</td>\n",
       "      <td>(.*?)BLOCK(.*?) ask(.*?)to replicate(.*?)to(.*?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>E26</td>\n",
       "      <td>[*]BLOCK* NameSystem[*]addStoredBlock: blockMa...</td>\n",
       "      <td>(.*?)BLOCK(.*?) NameSystem(.*?)addStoredBlock:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>E27</td>\n",
       "      <td>[*]BLOCK* NameSystem[*]addStoredBlock: Redunda...</td>\n",
       "      <td>(.*?)BLOCK(.*?) NameSystem(.*?)addStoredBlock:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>E28</td>\n",
       "      <td>[*]BLOCK* NameSystem[*]addStoredBlock: addStor...</td>\n",
       "      <td>(.*?)BLOCK(.*?) NameSystem(.*?)addStoredBlock:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>E29</td>\n",
       "      <td>[*]PendingReplicationMonitor timed out block[*]</td>\n",
       "      <td>(.*?)PendingReplicationMonitor timed out block...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId                                      EventTemplate  \\\n",
       "0       E1             [*]Adding an already existing block[*]   \n",
       "1       E2                   [*]Verification succeeded for[*]   \n",
       "2       E3                            [*]Served block[*]to[*]   \n",
       "3       E4             [*]Got exception while serving[*]to[*]   \n",
       "4       E5               [*]Receiving block[*]src:[*]dest:[*]   \n",
       "5       E6      [*]Received block[*]src:[*]dest:[*]of size[*]   \n",
       "6       E7              [*]writeBlock[*]received exception[*]   \n",
       "7       E8    [*]PacketResponder[*]for block[*]Interrupted[*]   \n",
       "8       E9              [*]Received block[*]of size[*]from[*]   \n",
       "9      E10                  [*]PacketResponder[*]Exception[*]   \n",
       "10     E11    [*]PacketResponder[*]for block[*]terminating[*]   \n",
       "11     E12         [*]:Exception writing block[*]to mirror[*]   \n",
       "12     E13             [*]Receiving empty packet for block[*]   \n",
       "13     E14          [*]Exception in receiveBlock for block[*]   \n",
       "14     E15  [*]Changing block file offset of block[*]from[...   \n",
       "15     E16                      [*]:Transmitted block[*]to[*]   \n",
       "16     E17               [*]:Failed to transfer[*]to[*]got[*]   \n",
       "17     E18       [*]Starting thread to transfer block[*]to[*]   \n",
       "18     E19                                 [*]Reopen Block[*]   \n",
       "19     E20  [*]Unexpected error trying to delete block[*]B...   \n",
       "20     E21                        [*]Deleting block[*]file[*]   \n",
       "21     E22           [*]BLOCK* NameSystem[*]allocateBlock:[*]   \n",
       "22     E23  [*]BLOCK* NameSystem[*]delete:[*]is added to i...   \n",
       "23     E24  [*]BLOCK* Removing block[*]from neededReplicat...   \n",
       "24     E25               [*]BLOCK* ask[*]to replicate[*]to[*]   \n",
       "25     E26  [*]BLOCK* NameSystem[*]addStoredBlock: blockMa...   \n",
       "26     E27  [*]BLOCK* NameSystem[*]addStoredBlock: Redunda...   \n",
       "27     E28  [*]BLOCK* NameSystem[*]addStoredBlock: addStor...   \n",
       "28     E29    [*]PendingReplicationMonitor timed out block[*]   \n",
       "\n",
       "                                        regex_pattern  \n",
       "0          (.*?)Adding an already existing block(.*?)  \n",
       "1                (.*?)Verification succeeded for(.*?)  \n",
       "2                       (.*?)Served block(.*?)to(.*?)  \n",
       "3        (.*?)Got exception while serving(.*?)to(.*?)  \n",
       "4        (.*?)Receiving block(.*?)src:(.*?)dest:(.*?)  \n",
       "5   (.*?)Received block(.*?)src:(.*?)dest:(.*?)of ...  \n",
       "6         (.*?)writeBlock(.*?)received exception(.*?)  \n",
       "7   (.*?)PacketResponder(.*?)for block(.*?)Interru...  \n",
       "8       (.*?)Received block(.*?)of size(.*?)from(.*?)  \n",
       "9             (.*?)PacketResponder(.*?)Exception(.*?)  \n",
       "10  (.*?)PacketResponder(.*?)for block(.*?)termina...  \n",
       "11   (.*?):Exception writing block(.*?)to mirror(.*?)  \n",
       "12         (.*?)Receiving empty packet for block(.*?)  \n",
       "13      (.*?)Exception in receiveBlock for block(.*?)  \n",
       "14  (.*?)Changing block file offset of block(.*?)f...  \n",
       "15                (.*?):Transmitted block(.*?)to(.*?)  \n",
       "16       (.*?):Failed to transfer(.*?)to(.*?)got(.*?)  \n",
       "17  (.*?)Starting thread to transfer block(.*?)to(...  \n",
       "18                             (.*?)Reopen Block(.*?)  \n",
       "19  (.*?)Unexpected error trying to delete block(....  \n",
       "20                  (.*?)Deleting block(.*?)file(.*?)  \n",
       "21  (.*?)BLOCK(.*?) NameSystem(.*?)allocateBlock:(...  \n",
       "22  (.*?)BLOCK(.*?) NameSystem(.*?)delete:(.*?)is ...  \n",
       "23  (.*?)BLOCK(.*?) Removing block(.*?)from needed...  \n",
       "24   (.*?)BLOCK(.*?) ask(.*?)to replicate(.*?)to(.*?)  \n",
       "25  (.*?)BLOCK(.*?) NameSystem(.*?)addStoredBlock:...  \n",
       "26  (.*?)BLOCK(.*?) NameSystem(.*?)addStoredBlock:...  \n",
       "27  (.*?)BLOCK(.*?) NameSystem(.*?)addStoredBlock:...  \n",
       "28  (.*?)PendingReplicationMonitor timed out block...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eid_df['regex_pattern'] = (\n",
    "    eid_df['EventTemplate']\n",
    "    .str.replace(']', '', regex=False)\n",
    "    .str.replace('[', '', regex=False)\n",
    "    .str.replace('*', '(.*?)', regex=False)\n",
    ")\n",
    "\n",
    "eid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7f6eb-0f2d-4275-a48b-3e7856fe178f",
   "metadata": {},
   "source": [
    "### 4. Parsing the log file using regex to create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0065de-a998-4f7f-bc03-3b75d93c876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date    Time  Pid Level                     Component  \\\n",
       "0  081109  203518  143  INFO      dfs.DataNode$DataXceiver   \n",
       "1  081109  203518   35  INFO              dfs.FSNamesystem   \n",
       "2  081109  203519  143  INFO      dfs.DataNode$DataXceiver   \n",
       "3  081109  203519  145  INFO      dfs.DataNode$DataXceiver   \n",
       "4  081109  203519  145  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             Content  \n",
       "0  Receiving block blk_-1608999687919862906 src: ...  \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...  \n",
       "2  Receiving block blk_-1608999687919862906 src: ...  \n",
       "3  Receiving block blk_-1608999687919862906 src: ...  \n",
       "4  PacketResponder 1 for block blk_-1608999687919...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.DataFrame(hdfs_head, columns=['raw_log'])\n",
    "\n",
    "# # Regex pattern\n",
    "regex_pattern = r'^(?P<Date>\\d{6})\\s+(?P<Time>\\d{6})\\s+(?P<Pid>\\d+)\\s+(?P<Level>\\w+)\\s+(?P<Component>[^:]+):\\s+(?P<Content>.*)$'\n",
    "\n",
    "# # Seperating into date, time logs\n",
    "df_base = df_raw['raw_log'].str.extract(regex_pattern)\n",
    "\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc83cc2-174b-4bf8-bb26-4fd4b373fc41",
   "metadata": {},
   "source": [
    "### 5. Using Regex to get event ID for each log entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb196b8-f985-49c6-8bbe-bf2f16f0da83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Combine all patterns into one giant regex using named groups\n",
    "# Format: (?P<E1>pattern1)|(?P<E2>pattern2)|...\n",
    "\n",
    "combined_regex_str = \"|\".join([f\"(?P<{eid}>{pat})\" for eid, pat in zip(eid_df['EventId'], eid_df['regex_pattern'])])\n",
    "combined_pattern = re.compile(combined_regex_str)\n",
    "\n",
    "# 2. Define a function to extract the name of the group that matched\n",
    "def get_event_id_fast(content):\n",
    "    match = combined_pattern.search(content)\n",
    "    if match:\n",
    "        # lastgroup returns the name of the named group that participated in the match\n",
    "        return match.lastgroup\n",
    "    return None\n",
    "\n",
    "# 3. Apply to the dataframe\n",
    "df_base['EventId'] = df_base['Content'].apply(get_event_id_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14d1e7-ef2f-4d46-8cb7-b5d998cd3d99",
   "metadata": {},
   "source": [
    "### 6. Getting the block ID as well as counting number of events per block id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3115f9-a517-4f4b-81ee-23881a5b70b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventId                   BlockId  E1  E2  E3  E4  E5  E6  E7  E8  E9  ...  \\\n",
      "0        blk_-1000002529962039464   0   0   0   0   3   0   0   0   3  ...   \n",
      "1         blk_-100000266894974466   0   0   6   3   3   0   0   0   3  ...   \n",
      "2        blk_-1000007292892887521   0   0   0   0   3   0   0   0   3  ...   \n",
      "3        blk_-1000014584150379967   0   1   6   3   3   0   0   0   3  ...   \n",
      "4        blk_-1000028658773048709   0   0   0   0   3   0   0   0   3  ...   \n",
      "\n",
      "EventId  E20  E21  E22  E23  E24  E25  E26  E27  E28  E29  \n",
      "0          0    0    1    0    0    0    3    0    0    0  \n",
      "1          0    3    1    3    0    0    3    0    0    0  \n",
      "2          0    0    1    0    0    0    3    0    0    0  \n",
      "3          0    3    1    3    0    0    3    0    0    0  \n",
      "4          0    3    1    3    0    0    3    0    0    0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "df_base['BlockId'] = df_base['Content'].str.extract(r'(blk_-?\\d+)')\n",
    "\n",
    "# Generate all Events into columns\n",
    "e_matrix = pd.crosstab(df_base['BlockId'], df_base['EventId'])\n",
    "all_event_ids = [f'E{i}' for i in range(1, 30)]\n",
    "e_matrix = e_matrix.reindex(columns=all_event_ids, fill_value=0)\n",
    "\n",
    "e_matrix = e_matrix.reset_index().fillna(0)\n",
    "\n",
    "print(e_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c462a676-d368-4dc8-b7b6-b0141b06ad7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    BlockId MaxLevel\n",
      "0  blk_-1000002529962039464     INFO\n",
      "1   blk_-100000266894974466     WARN\n",
      "2  blk_-1000007292892887521     INFO\n",
      "3  blk_-1000014584150379967     WARN\n",
      "4  blk_-1000028658773048709     INFO\n",
      "                    BlockId MaxLevel  MaxLevelNum\n",
      "0  blk_-1000002529962039464     INFO            1\n",
      "1   blk_-100000266894974466     WARN            2\n",
      "2  blk_-1000007292892887521     INFO            1\n",
      "3  blk_-1000014584150379967     WARN            2\n",
      "4  blk_-1000028658773048709     INFO            1\n",
      "                    BlockId  E1  E2  E3  E4  E5  E6  E7  E8  E9  ...  E24  \\\n",
      "0  blk_-1000002529962039464   0   0   0   0   3   0   0   0   3  ...    0   \n",
      "1   blk_-100000266894974466   0   0   6   3   3   0   0   0   3  ...    0   \n",
      "2  blk_-1000007292892887521   0   0   0   0   3   0   0   0   3  ...    0   \n",
      "3  blk_-1000014584150379967   0   1   6   3   3   0   0   0   3  ...    0   \n",
      "4  blk_-1000028658773048709   0   0   0   0   3   0   0   0   3  ...    0   \n",
      "\n",
      "   E25  E26  E27  E28  E29  MaxLevel_x  MaxLevelNum_x  MaxLevel_y  \\\n",
      "0    0    3    0    0    0        INFO              1        INFO   \n",
      "1    0    3    0    0    0        WARN              2        WARN   \n",
      "2    0    3    0    0    0        INFO              1        INFO   \n",
      "3    0    3    0    0    0        WARN              2        WARN   \n",
      "4    0    3    0    0    0        INFO              1        INFO   \n",
      "\n",
      "   MaxLevelNum_y  \n",
      "0              1  \n",
      "1              2  \n",
      "2              1  \n",
      "3              2  \n",
      "4              1  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get Highest Level per blockId\n",
    "block_severity = df_base.groupby('BlockId', as_index=False)['Level'].max()\n",
    "block_severity.rename(columns={'Level': 'MaxLevel'}, inplace=True)\n",
    "\n",
    "print(block_severity.head())\n",
    "\n",
    "custom_map = {'INFO': 1, 'WARN': 2, 'ERROR': 3, 'FATAL': 100}\n",
    "block_severity['MaxLevelNum'] = block_severity['MaxLevel'].map(custom_map)\n",
    "print(block_severity.head())\n",
    "\n",
    "# Merge with highest level per block\n",
    "e_matrix = pd.merge(e_matrix, block_severity, on='BlockId', how='inner')\n",
    "\n",
    "e_matrix.to_csv('test_occurrence.csv')\n",
    "\n",
    "print(e_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27057ef-e6cf-48eb-b9af-efac9dda51a7",
   "metadata": {},
   "source": [
    "### 7. Training and testing the cleaned data provided to have a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351c7bf3-2a57-4454-93da-9d9641df63c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3387     15]\n",
      " [    18 111593]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fail       0.99      1.00      1.00      3402\n",
      "     Success       1.00      1.00      1.00    111611\n",
      "\n",
      "    accuracy                           1.00    115013\n",
      "   macro avg       1.00      1.00      1.00    115013\n",
      "weighted avg       1.00      1.00      1.00    115013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# declaring some var\n",
    "scaled = StandardScaler()\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "train_df = pd.read_csv('Event_occurrence_matrix.csv')\n",
    "\n",
    "# splitting data\n",
    "x = train_df.drop(['BlockId','Label','Type'],axis=1)\n",
    "y = train_df.Label\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.2,random_state=33)\n",
    "\n",
    "# Training data\n",
    "tree.fit(xtrain, ytrain)\n",
    "y_pred_tree = tree.predict(xtest)\n",
    "\n",
    "# Testing data\n",
    "print(confusion_matrix(ytest, y_pred_tree))\n",
    "print(classification_report(ytest, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fad9e2a-7cc7-43be-98e2-e0a0063907fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- E9 <= 2.50\n",
      "|   |--- class: Fail\n",
      "|--- E9 >  2.50\n",
      "|   |--- E20 <= 0.50\n",
      "|   |   |--- E26 <= 3.50\n",
      "|   |   |   |--- E27 <= 0.50\n",
      "|   |   |   |   |--- E13 <= 1.50\n",
      "|   |   |   |   |   |--- class: Success\n",
      "|   |   |   |   |--- E13 >  1.50\n",
      "|   |   |   |   |   |--- class: Fail\n",
      "|   |   |   |--- E27 >  0.50\n",
      "|   |   |   |   |--- E4 <= 9.50\n",
      "|   |   |   |   |   |--- class: Fail\n",
      "|   |   |   |   |--- E4 >  9.50\n",
      "|   |   |   |   |   |--- class: Success\n",
      "|   |   |--- E26 >  3.50\n",
      "|   |   |   |--- E23 <= 1.00\n",
      "|   |   |   |   |--- E29 <= 0.50\n",
      "|   |   |   |   |   |--- class: Success\n",
      "|   |   |   |   |--- E29 >  0.50\n",
      "|   |   |   |   |   |--- class: Fail\n",
      "|   |   |   |--- E23 >  1.00\n",
      "|   |   |   |   |--- E2 <= 3.50\n",
      "|   |   |   |   |   |--- class: Fail\n",
      "|   |   |   |   |--- E2 >  3.50\n",
      "|   |   |   |   |   |--- class: Success\n",
      "|   |--- E20 >  0.50\n",
      "|   |   |--- E3 <= 7.50\n",
      "|   |   |   |--- E23 <= 4.50\n",
      "|   |   |   |   |--- class: Fail\n",
      "|   |   |   |--- E23 >  4.50\n",
      "|   |   |   |   |--- class: Success\n",
      "|   |   |--- E3 >  7.50\n",
      "|   |   |   |--- E4 <= 3.50\n",
      "|   |   |   |   |--- class: Fail\n",
      "|   |   |   |--- E4 >  3.50\n",
      "|   |   |   |   |--- E20 <= 1.50\n",
      "|   |   |   |   |   |--- class: Success\n",
      "|   |   |   |   |--- E20 >  1.50\n",
      "|   |   |   |   |   |--- class: Fail\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "# Print the rules the model is using\n",
    "m = tree.predict(e_matrix.drop(['BlockId','MaxLevel','MaxLevelNum'],axis=1))\n",
    "\n",
    "tree_rules = export_text(tree, feature_names=list(x.columns))\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d492c24-ee8d-4ffc-bf57-a3eacaeb1b63",
   "metadata": {},
   "source": [
    "### 8.Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c136f-1bbf-40eb-af7d-a88075b94641",
   "metadata": {},
   "source": [
    "### 9.New Way to format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "995362ba-baac-4063-a975-df4342b2a654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>E5</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>2008-11-09 20:35:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "      <td>E22</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>2008-11-09 20:35:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>E5</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>2008-11-09 20:35:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>E5</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>2008-11-09 20:35:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "      <td>E11</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>2008-11-09 20:35:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date    Time  Pid Level                     Component  \\\n",
       "0  081109  203518  143  INFO      dfs.DataNode$DataXceiver   \n",
       "1  081109  203518   35  INFO              dfs.FSNamesystem   \n",
       "2  081109  203519  143  INFO      dfs.DataNode$DataXceiver   \n",
       "3  081109  203519  145  INFO      dfs.DataNode$DataXceiver   \n",
       "4  081109  203519  145  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             Content EventId  \\\n",
       "0  Receiving block blk_-1608999687919862906 src: ...      E5   \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...     E22   \n",
       "2  Receiving block blk_-1608999687919862906 src: ...      E5   \n",
       "3  Receiving block blk_-1608999687919862906 src: ...      E5   \n",
       "4  PacketResponder 1 for block blk_-1608999687919...     E11   \n",
       "\n",
       "                    BlockId           Timestamp  \n",
       "0  blk_-1608999687919862906 2008-11-09 20:35:18  \n",
       "1  blk_-1608999687919862906 2008-11-09 20:35:18  \n",
       "2  blk_-1608999687919862906 2008-11-09 20:35:19  \n",
       "3  blk_-1608999687919862906 2008-11-09 20:35:19  \n",
       "4  blk_-1608999687919862906 2008-11-09 20:35:19  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format data to have timestamp column and interval\n",
    "\n",
    "df_base['Timestamp'] = pd.to_datetime(\n",
    "    df_base['Date'] + ' ' + df_base['Time'], \n",
    "    format='%y%m%d %H%M%S'\n",
    ")\n",
    "\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8d731-aaeb-443a-ae09-a49e100f4070",
   "metadata": {},
   "source": [
    "### 9. Formatting the data before export by comparing with provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36971abe-c64a-4e01-afee-86cb80b4b676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koh bock chow\\AppData\\Local\\Temp\\ipykernel_22076\\1700156369.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  block_seq = df_base.groupby('BlockId').apply(get_sequences).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    BlockId  \\\n",
      "0  blk_-1000002529962039464   \n",
      "1   blk_-100000266894974466   \n",
      "2  blk_-1000007292892887521   \n",
      "3  blk_-1000014584150379967   \n",
      "4  blk_-1000028658773048709   \n",
      "\n",
      "                                            Features  \\\n",
      "0  [E5, E5, E5, E22, E11, E9, E11, E9, E26, E26, ...   \n",
      "1  [E22, E5, E5, E5, E26, E26, E26, E11, E9, E11,...   \n",
      "2  [E5, E5, E22, E5, E11, E9, E11, E9, E11, E9, E...   \n",
      "3  [E5, E22, E5, E5, E26, E26, E26, E11, E9, E11,...   \n",
      "4  [E5, E5, E5, E22, E11, E9, E11, E9, E11, E9, E...   \n",
      "\n",
      "                                       TimeIntervals  Latency  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, ...      3.0  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 35.0, 0.0, 0.0, 0.0, 0.0,...  30958.0  \n",
      "2  [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...      2.0  \n",
      "3  [0.0, 1.0, 1.0, 1.0, 38.0, 0.0, 0.0, 0.0, 0.0,...  31460.0  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 17.0, 0.0, 0.0, 0.0, 0.0,...  24078.0  \n"
     ]
    }
   ],
   "source": [
    "# 2. Sort by BlockId and then Timestamp to ensure the sequence is chronological\n",
    "df_base = df_base.sort_values(['BlockId', 'Timestamp'])\n",
    "\n",
    "# 3. Aggregate EventIds into a list and calculate time intervals\n",
    "def get_sequences(group):\n",
    "    # Get the sequence of Event IDs\n",
    "    features = group['EventId'].tolist()\n",
    "    \n",
    "    # Calculate difference between consecutive timestamps in seconds\n",
    "    intervals = group['Timestamp'].diff().dt.total_seconds().fillna(0).tolist()\n",
    "    \n",
    "    # Sum of all intervals\n",
    "    latency = sum(intervals)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Features': features,\n",
    "        'TimeIntervals': intervals,\n",
    "        'Latency': latency\n",
    "    })\n",
    "\n",
    "# 4. Apply to get the final result\n",
    "block_seq = df_base.groupby('BlockId').apply(get_sequences).reset_index()\n",
    "\n",
    "block_seq.to_csv('test_exp_traces.csv')\n",
    "\n",
    "print(block_seq.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7011957a-e1d8-45bf-b62e-455df6f0eaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_seq.Features.apply(tuple).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d489772e-e3d2-4e13-8158-741ab9b3d710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "      <th>Type</th>\n",
       "      <th>Features</th>\n",
       "      <th>TimeInterval</th>\n",
       "      <th>Latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Fail</td>\n",
       "      <td>21.0</td>\n",
       "      <td>[E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E3,E26,E26,...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>50448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 48.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>50583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId    Label  Type  \\\n",
       "0  blk_-1608999687919862906  Success   NaN   \n",
       "1   blk_7503483334202473044  Success   NaN   \n",
       "2  blk_-3544583377289625738     Fail  21.0   \n",
       "3  blk_-9073992586687739851  Success   NaN   \n",
       "4   blk_7854771516489510256  Success   NaN   \n",
       "\n",
       "                                            Features  \\\n",
       "0  [E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...   \n",
       "1  [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...   \n",
       "2  [E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E3,E26,E26,...   \n",
       "3  [E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...   \n",
       "4  [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...   \n",
       "\n",
       "                                        TimeInterval  Latency  \n",
       "0  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     3802  \n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     3802  \n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...     3797  \n",
       "3  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    50448  \n",
       "4  [0.0, 0.0, 1.0, 48.0, 0.0, 0.0, 0.0, 0.0, 0.0,...    50583  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = ['anomaly_label.csv','Event_occurrence_matrix.csv','Event_traces.csv','HDFS.log_templates.csv']\n",
    "\n",
    "pandas_list = []\n",
    "\n",
    "for file in files:\n",
    "    pandas_list.append(pd.read_csv(file))\n",
    "\n",
    "pandas_list[2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2824853-b453-429d-8155-a750ff49111b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18373"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_list[2].Features.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c323882-e72c-40cb-acc5-92425a3bf2b8",
   "metadata": {},
   "source": [
    "## 10. HDFS V2 Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c095a71c-fde1-42d9-855b-435ad23730af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2015-12-03 14:37:47,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n",
      "\n",
      "1 /************************************************************\n",
      "\n",
      "2 STARTUP_MSG: Starting DataNode\n",
      "\n",
      "3 STARTUP_MSG:   host = mesos-master-1/10.10.34.11\n",
      "\n",
      "4 STARTUP_MSG:   args = []\n",
      "\n",
      "5 STARTUP_MSG:   version = 2.7.1\n",
      "\n",
      "6 STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar\n",
      "\n",
      "7 STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z\n",
      "\n",
      "8 STARTUP_MSG:   java = 1.7.0_79\n",
      "\n",
      "9 ************************************************************/\n",
      "\n",
      "10 2015-12-03 14:37:47,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "\n",
      "11 2015-12-03 14:37:48,253 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "\n",
      "12 2015-12-03 14:37:48,315 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n",
      "\n",
      "13 2015-12-03 14:37:48,315 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n",
      "\n",
      "14 2015-12-03 14:37:48,319 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\n",
      "\n",
      "15 2015-12-03 14:37:48,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is mesos-master-1\n",
      "\n",
      "16 2015-12-03 14:37:48,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0\n",
      "\n",
      "17 2015-12-03 14:37:48,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010\n",
      "\n",
      "18 2015-12-03 14:37:48,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s\n",
      "\n",
      "19 2015-12-03 14:37:48,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "hdfs2_folder = './HDFS_v2_node_logs'\n",
    "hdfs2_csv_folder = hdfs2_folder + '/hdfsv2_csv'\n",
    "hdfs2_files = [a for a in os.listdir(hdfs2_folder) if '.log' in a]\n",
    "\n",
    "with open(hdfs2_folder+'/'+hdfs2_files[0],'r') as reader:\n",
    "    hdfs2_df0 = reader.readlines()\n",
    "\n",
    "# Check how the log is formatted before parsing and cleaning\n",
    "\n",
    "for i,line in enumerate(hdfs2_df0[:20]):\n",
    "    print(i,line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4083df38-8b75-4c84-85da-18c4646a5bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:47,618</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNode</td>\n",
       "      <td>registered UNIX signal handlers for [TERM, HUP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:48,253</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsConfig</td>\n",
       "      <td>loaded properties from hadoop-metrics2.properties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:48,315</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsSystemImpl</td>\n",
       "      <td>Scheduled snapshot period at 10 second(s).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:48,315</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsSystemImpl</td>\n",
       "      <td>DataNode metrics system started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:48,319</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.BlockSc...</td>\n",
       "      <td>Initialized block scanner with targetBytesPerS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Time Level  \\\n",
       "0  2015-12-03  14:37:47,618  INFO   \n",
       "1  2015-12-03  14:37:48,253  INFO   \n",
       "2  2015-12-03  14:37:48,315  INFO   \n",
       "3  2015-12-03  14:37:48,315  INFO   \n",
       "4  2015-12-03  14:37:48,319  INFO   \n",
       "\n",
       "                                           Component  \\\n",
       "0    org.apache.hadoop.hdfs.server.datanode.DataNode   \n",
       "1      org.apache.hadoop.metrics2.impl.MetricsConfig   \n",
       "2  org.apache.hadoop.metrics2.impl.MetricsSystemImpl   \n",
       "3  org.apache.hadoop.metrics2.impl.MetricsSystemImpl   \n",
       "4  org.apache.hadoop.hdfs.server.datanode.BlockSc...   \n",
       "\n",
       "                                             Content  \n",
       "0  registered UNIX signal handlers for [TERM, HUP...  \n",
       "1  loaded properties from hadoop-metrics2.properties  \n",
       "2         Scheduled snapshot period at 10 second(s).  \n",
       "3                    DataNode metrics system started  \n",
       "4  Initialized block scanner with targetBytesPerS...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Omitting the front part of the dataset since it does not follow rest of the log format\n",
    "df_raw = pd.DataFrame(hdfs2_df0[10:],columns=['raw'])\n",
    "log_reg_pattern = r'^(?P<Date>\\d{4}-\\d{2}-\\d{2})\\s+(?P<Time>\\d{2}:\\d{2}:\\d{2},\\d{3})\\s+(?P<Level>[A-Z]+)\\s+(?P<Component>[\\w\\.]+):\\s+(?P<Content>.*)$'\n",
    "\n",
    "# Seperating the values to columns in dataframe\n",
    "df_base = df_raw['raw'].str.extract(log_reg_pattern)\n",
    "\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5cec1-af4b-4019-8831-a349f6bfd077",
   "metadata": {},
   "source": [
    "### 10.1 Event Identification and storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ebb761-1e41-46d8-b863-b6f8b868ef0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: (2614796, 5)\n",
      "Database describe:               Date          Time    Level  \\\n",
      "count      2611375       2611375  2611375   \n",
      "unique         579       1015724        3   \n",
      "top     2016-10-06  21:44:05,989     INFO   \n",
      "freq        456424           232  2610681   \n",
      "\n",
      "                                              Component  \\\n",
      "count                                           2611375   \n",
      "unique                                               19   \n",
      "top     org.apache.hadoop.hdfs.server.datanode.DataNode   \n",
      "freq                                            1060635   \n",
      "\n",
      "                                                  Content  \n",
      "count                                             2611375  \n",
      "unique                                            2594943  \n",
      "top     Got finalize command for block pool BP-1088411...  \n",
      "freq                                                 2300  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2614796 entries, 0 to 2614795\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Dtype \n",
      "---  ------     ----- \n",
      " 0   Date       object\n",
      " 1   Time       object\n",
      " 2   Level      object\n",
      " 3   Component  object\n",
      " 4   Content    object\n",
      "dtypes: object(5)\n",
      "memory usage: 99.7+ MB\n",
      "Database info: None\n",
      "Empty values: Date         3421\n",
      "Time         3421\n",
      "Level        3421\n",
      "Component    3421\n",
      "Content      3421\n",
      "dtype: int64\n",
      "Number of unique content: 2594943\n"
     ]
    }
   ],
   "source": [
    "print('Size of dataset:',df_base.shape)\n",
    "print('Database describe:', df_base.describe())\n",
    "print('Database info:', df_base.info())\n",
    "print('Empty values:', df_base.isna().sum())\n",
    "print('Number of unique content:',df_base.Content.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59cf3bda-df72-4c6a-9f66-e219f661b8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:47,618</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNode</td>\n",
       "      <td>registered UNIX signal handlers for [TERM, HUP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:48,253</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsConfig</td>\n",
       "      <td>loaded properties from hadoop-metrics2.properties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:48,315</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsSystemImpl</td>\n",
       "      <td>Scheduled snapshot period at 10 second(s).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:48,315</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsSystemImpl</td>\n",
       "      <td>DataNode metrics system started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>14:37:48,319</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.BlockSc...</td>\n",
       "      <td>Initialized block scanner with targetBytesPerS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614791</th>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>12:32:00,003</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNode</td>\n",
       "      <td>Got finalize command for block pool BP-1088411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614792</th>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>13:16:34,202</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.Directo...</td>\n",
       "      <td>BlockPool BP-108841162-10.10.34.11-14400743609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614793</th>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>18:32:00,024</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNode</td>\n",
       "      <td>Successfully sent block report 0x633af28a2a98c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614794</th>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>18:32:00,025</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNode</td>\n",
       "      <td>Got finalize command for block pool BP-1088411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614795</th>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>19:16:34,192</td>\n",
       "      <td>INFO</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.Directo...</td>\n",
       "      <td>BlockPool BP-108841162-10.10.34.11-14400743609...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2611375 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date          Time Level  \\\n",
       "0        2015-12-03  14:37:47,618  INFO   \n",
       "1        2015-12-03  14:37:48,253  INFO   \n",
       "2        2015-12-03  14:37:48,315  INFO   \n",
       "3        2015-12-03  14:37:48,315  INFO   \n",
       "4        2015-12-03  14:37:48,319  INFO   \n",
       "...             ...           ...   ...   \n",
       "2614791  2017-07-31  12:32:00,003  INFO   \n",
       "2614792  2017-07-31  13:16:34,202  INFO   \n",
       "2614793  2017-07-31  18:32:00,024  INFO   \n",
       "2614794  2017-07-31  18:32:00,025  INFO   \n",
       "2614795  2017-07-31  19:16:34,192  INFO   \n",
       "\n",
       "                                                 Component  \\\n",
       "0          org.apache.hadoop.hdfs.server.datanode.DataNode   \n",
       "1            org.apache.hadoop.metrics2.impl.MetricsConfig   \n",
       "2        org.apache.hadoop.metrics2.impl.MetricsSystemImpl   \n",
       "3        org.apache.hadoop.metrics2.impl.MetricsSystemImpl   \n",
       "4        org.apache.hadoop.hdfs.server.datanode.BlockSc...   \n",
       "...                                                    ...   \n",
       "2614791    org.apache.hadoop.hdfs.server.datanode.DataNode   \n",
       "2614792  org.apache.hadoop.hdfs.server.datanode.Directo...   \n",
       "2614793    org.apache.hadoop.hdfs.server.datanode.DataNode   \n",
       "2614794    org.apache.hadoop.hdfs.server.datanode.DataNode   \n",
       "2614795  org.apache.hadoop.hdfs.server.datanode.Directo...   \n",
       "\n",
       "                                                   Content  \n",
       "0        registered UNIX signal handlers for [TERM, HUP...  \n",
       "1        loaded properties from hadoop-metrics2.properties  \n",
       "2               Scheduled snapshot period at 10 second(s).  \n",
       "3                          DataNode metrics system started  \n",
       "4        Initialized block scanner with targetBytesPerS...  \n",
       "...                                                    ...  \n",
       "2614791  Got finalize command for block pool BP-1088411...  \n",
       "2614792  BlockPool BP-108841162-10.10.34.11-14400743609...  \n",
       "2614793  Successfully sent block report 0x633af28a2a98c...  \n",
       "2614794  Got finalize command for block pool BP-1088411...  \n",
       "2614795  BlockPool BP-108841162-10.10.34.11-14400743609...  \n",
       "\n",
       "[2611375 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Drop null values\n",
    "df_base.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48eff504-e5ad-444e-8f27-be0f0b6f33ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Time', 'Level', 'Component', 'Content'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Considering content has alot of unique data and there is no event list to reference off, it is best to try and create one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63033312-7725-4778-8d6c-440278d38f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print('1234567890'[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6a3b5-e2a6-4e95-a087-abfad7a25f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
